{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIYE3rbYr5Lr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "import shap\n",
    "import transformers\n",
    "import pandas as pd\n",
    "# Create class for data preparation\n",
    "class SimpleDataset:\n",
    "    def __init__(self, tokenized_texts):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.tokenized_texts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWGCNHjbr5Lt"
   },
   "outputs": [],
   "source": [
    "fulldata=pd.read_csv('fulldata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSsGDmWYr5Lu"
   },
   "outputs": [],
   "source": [
    "def find_sus(label):\n",
    "    if label.find('Sustaining proliferative signaling') == -1:\n",
    "        return 0\n",
    "    else: \n",
    "        return 1\n",
    "\n",
    "def find_data(label):\n",
    "    if label.find('Resisting cell death') != -1:\n",
    "        return 1\n",
    "    else: \n",
    "        if label.find('Sustaining proliferative signaling') != -1:\n",
    "            return 1\n",
    "        else:\n",
    "            if label.find('Tumor promoting inflammation') != -1:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03jeWmoYr5Lu"
   },
   "outputs": [],
   "source": [
    "fulldata['label1']=fulldata['label'].apply(find_sus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6jhdytFr5Lu"
   },
   "outputs": [],
   "source": [
    "fulldata['label2']=fulldata['label'].apply(find_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAhA_mrLr5Lv"
   },
   "outputs": [],
   "source": [
    "#newdata=fulldata[fulldata['label2']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pDSTUZfr5Lv"
   },
   "outputs": [],
   "source": [
    "dataset=pd.DataFrame(data={'text':[],'label':[]})\n",
    "dataset['text']=fulldata['text']\n",
    "dataset['label']=fulldata['label1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_ML51GBr5Lv"
   },
   "outputs": [],
   "source": [
    "dataset.to_csv('fulldataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_efKld7Jr5Lw"
   },
   "outputs": [],
   "source": [
    "'''hocdataset=pd.DataFrame(data={'text':[],'label':[]})\n",
    "hocdataset['text']=newdata['text']\n",
    "hocdataset['label']=newdata['label1']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2k0Wbslr5Lw"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dz4bUH_dr5Lw",
    "outputId": "dc528903-b91a-4bc6-fc76-5819b77d4d21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'train.to_csv('train.csv',index=False)\\ntest.to_csv('test.csv',index=False)\\nnp.save('test.npy',test)\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''train.to_csv('train.csv',index=False)\n",
    "test.to_csv('test.csv',index=False)\n",
    "np.save('test.npy',test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgn0KCL-r5Lx"
   },
   "outputs": [],
   "source": [
    "train.to_csv('fulltrain.csv',index=False)\n",
    "test.to_csv('fulltest.csv',index=False)\n",
    "np.save('fulltest.npy',test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpF_4ozUr5Lx",
    "outputId": "d6e24923-a40b-4350-cc1a-2f29a5134ac9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from datasets import load_dataset\\nraw_datasets=load_dataset('csv',data_files={'train': 'train.csv',\\n                                              'test':'test.csv'})\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from datasets import load_dataset\n",
    "raw_datasets=load_dataset('csv',data_files={'train': 'train.csv',\n",
    "                                              'test':'test.csv'})'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKxGQ3i7r5Lx",
    "outputId": "26be58ac-0bec-4363-86f6-c22daa94b450"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d6675006ef2dae1a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\RmmLeo10\\.cache\\huggingface\\datasets\\csv\\default-d6675006ef2dae1a\\0.0.0\\6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 1002.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\RmmLeo10\\.cache\\huggingface\\datasets\\csv\\default-d6675006ef2dae1a\\0.0.0\\6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 125.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets=load_dataset('csv',data_files={'train': 'fulltrain.csv',\n",
    "                                              'test':'fulltest.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VV07RltXr5Ly"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "\n",
    "# Create class for data preparation\n",
    "class SimpleDataset:\n",
    "    def __init__(self, tokenized_texts):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts[\"input_ids\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.tokenized_texts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBxs_Vxlr5Ly",
    "outputId": "27e49a3f-1165-4cad-d5dd-bbcced8a1dc8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\RmmLeo10/.cache\\huggingface\\transformers\\0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\RmmLeo10/.cache\\huggingface\\transformers\\75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\RmmLeo10/.cache\\huggingface\\transformers\\8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\RmmLeo10/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file distilbert-base-uncased-finetuned-sst-2-english\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file distilbert-base-uncased-finetuned-sst-2-english\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "tokenizer_name = \"distilbert-base-uncased\"\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(tokenizer_name)\n",
    "model = transformers.DistilBertForSequenceClassification.from_pretrained(model_name).to('cuda')\n",
    "trainer = Trainer(model=model)\n",
    "#pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True,device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwGZzm-sr5Ly",
    "outputId": "ac0d9447-7786-4e1b-a240-0e506ed0b505"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.28ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.70ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1n4DakjTr5Ly"
   },
   "outputs": [],
   "source": [
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "full_eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arwy1Shjr5Lz",
    "outputId": "ae5bef40-01cd-46a9-80b3-c249902708e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5X2mx2vr5Lz",
    "outputId": "5a828a55-738e-47b3-bbd0-a8a46492d710"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate = 2e-5,\n",
    "    #batch_size = 32,\n",
    "    warmup_steps= 0,\n",
    "    #max_seq_length = 128,\n",
    "    num_train_epochs = 5.0 ,\n",
    "    weight_decay=0.01 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMfh7g46r5Lz"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args, train_dataset=full_train_dataset, eval_dataset=full_eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0_8QRjPr5Lz",
    "outputId": "6903a41b-f153-4967-9af1-6ca5836d282b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "***** Running training *****\n",
      "  Num examples = 1481\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 930\n",
      " 54%|█████▍    | 500/930 [00:45<00:39, 10.91it/s]Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2597, 'learning_rate': 9.24731182795699e-06, 'epoch': 2.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "100%|█████████▉| 929/930 [01:26<00:00, 10.66it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 930/930 [01:26<00:00, 10.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 86.701, 'train_samples_per_second': 85.408, 'train_steps_per_second': 10.727, 'train_loss': 0.15510610867572086, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=930, training_loss=0.15510610867572086, metrics={'train_runtime': 86.701, 'train_samples_per_second': 85.408, 'train_steps_per_second': 10.727, 'train_loss': 0.15510610867572086, 'epoch': 5.0})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHCBVqUMr5Lz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-n2O1AFr5Lz",
    "outputId": "815cc8c9-ecb3-4bdc-8016-2305bbfdf511"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 371\n",
      "  Batch size = 8\n",
      "100%|██████████| 47/47 [00:01<00:00, 35.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6709614396095276,\n",
       " 'eval_accuracy': 0.889487870619946,\n",
       " 'eval_runtime': 1.3752,\n",
       " 'eval_samples_per_second': 269.774,\n",
       " 'eval_steps_per_second': 34.176}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_train_dataset,\n",
    "    eval_dataset=full_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbYCm4lar5L0",
    "outputId": "075c7f84-ef6a-4eeb-bfe7-31cbe5d82069"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:\\RMM\\Medical1\\distilbert-base-uncased-finetuned-sst-2-english2\n",
      "Configuration saved in C:\\RMM\\Medical1\\distilbert-base-uncased-finetuned-sst-2-english2\\config.json\n",
      "Model weights saved in C:\\RMM\\Medical1\\distilbert-base-uncased-finetuned-sst-2-english2\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('C:\\RMM\\Medical1\\distilbert-base-uncased-finetuned-sst-2-english2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ohq3-hb8r5L0",
    "outputId": "dc573995-3fbc-4e20-ddb9-e1b9c29c6f65"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file C:\\RMM\\Medical1\\distilbert-base-uncased-finetuned-sst-2-english2\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file C:\\RMM\\Medical1\\distilbert-base-uncased-finetuned-sst-2-english2\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at C:\\RMM\\Medical1\\distilbert-base-uncased-finetuned-sst-2-english2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertConfig, AutoModelForTokenClassification\n",
    "\n",
    "# retreive the saved model \n",
    "model = DistilBertForSequenceClassification.from_pretrained('C:\\RMM\\Medical1\\distilbert-base-uncased-finetuned-sst-2-english2', \n",
    "                                                        local_files_only=True)\n",
    "\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvx24ayhr5L0"
   },
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "pipe=TextClassificationPipeline(tokenizer=tokenizer,model=model,device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRvjE4KQr5L0"
   },
   "outputs": [],
   "source": [
    "cor_x = np.load(\"fulltest.npy\",allow_pickle=True)\n",
    "cor_reviews = [review[0] for review in cor_x]\n",
    "cor_labels = [review[1] for review in cor_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVIMewGOr5L0"
   },
   "outputs": [],
   "source": [
    "def score_and_visualize(text):\n",
    "  prediction = pipe([text])\n",
    "  print(prediction[0])\n",
    "\n",
    "  explainer = shap.Explainer(pipe)\n",
    "  shap_values = explainer([text])\n",
    "\n",
    "  shap.plots.text(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWflN_36r5L0"
   },
   "outputs": [],
   "source": [
    "def mask_top_k(k, pred_label_no_mask, values, returned_tokens):\n",
    "    \"\"\"\n",
    "    masked the k tokens that have the max shap values\n",
    "    :param k: specify the largest k value\n",
    "    :param values: shap values\n",
    "    :param returned_tokens: a list of tokens\n",
    "    :return: review, which is a str constructed from a list words\n",
    "    \"\"\"\n",
    "    shap_values_neg, shap_values_pos = zip(*values)\n",
    "    values = shap_values_neg if pred_label_no_mask==0 else shap_values_pos\n",
    "    # print(values)\n",
    "    values = np.array(values)\n",
    "    # ids_top_k = np.argpartition(values, -k)[-k:]\n",
    "    ids_top_k = (-values).argsort()[:k]\n",
    "    for idx in ids_top_k:\n",
    "        # print(idx)\n",
    "        returned_tokens[idx] = \"[UNK] \"\n",
    "    masked_review = \"\".join(returned_tokens)\n",
    "    # print(masked_review)\n",
    "    return masked_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYjJFNLHr5L0"
   },
   "outputs": [],
   "source": [
    "def predict_label(pipe, masked_review):\n",
    "    \"\"\"\n",
    "    predict the label for the masked_review\n",
    "    :param pipe: pipeline\n",
    "    :param masked_review: string\n",
    "    :return: 0 or 1, indicating the label\n",
    "    \"\"\"\n",
    "    prediction = pipe([masked_review])\n",
    "    if prediction[0]['label'] == 'NEGATIVE':\n",
    "            neg_score = prediction[0][\"score\"]\n",
    "            pos_score=1-prediction[0][\"score\"]\n",
    "    else:\n",
    "            pos_score = prediction[0][\"score\"] \n",
    "            neg_score = 1-prediction[0][\"score\"]       \n",
    "    pred_label = 0 if prediction[0]['label'] == 'NEGATIVE'  else 1\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vg28BQx3r5L0"
   },
   "outputs": [],
   "source": [
    "def model_prediction_gpu(x):\n",
    "    tv = torch.tensor([tokenizer.encode(v, padding='max_length', \n",
    "                                        max_length=90, truncation=True) for v in x]).cuda()\n",
    "    attention_mask = (tv!=0).type(torch.int64).cuda()\n",
    "    outputs = model(tv, attention_mask=attention_mask)[0]\n",
    "    scores = torch.nn.Softmax(dim=-1)(outputs)\n",
    "    val = torch.logit(scores).detach().cpu().numpy()\n",
    "\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcI9I5Tar5L0",
    "outputId": "9367aaef-edc6-4cf9-9e2c-6d06bbf0f176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 0-th review\n",
      "process 1-th review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (671 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 2-th review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 3-th review\n",
      "process 4-th review\n",
      "process 5-th review\n",
      "process 6-th review\n",
      "process 7-th review\n",
      "process 8-th review\n",
      "process 9-th review\n",
      "process 10-th review\n",
      "process 11-th review\n",
      "process 12-th review\n",
      "process 13-th review\n",
      "process 14-th review\n",
      "process 15-th review\n",
      "process 16-th review\n",
      "process 17-th review\n",
      "process 18-th review\n",
      "process 19-th review\n",
      "process 20-th review\n",
      "process 21-th review\n",
      "process 22-th review\n",
      "process 23-th review\n",
      "process 24-th review\n",
      "process 25-th review\n",
      "process 26-th review\n",
      "process 27-th review\n",
      "process 28-th review\n",
      "process 29-th review\n",
      "process 30-th review\n",
      "process 31-th review\n",
      "process 32-th review\n",
      "process 33-th review\n",
      "process 34-th review\n",
      "process 35-th review\n",
      "process 36-th review\n",
      "process 37-th review\n",
      "process 38-th review\n",
      "process 39-th review\n",
      "process 40-th review\n",
      "process 41-th review\n",
      "process 42-th review\n",
      "process 43-th review\n",
      "process 44-th review\n",
      "process 45-th review\n",
      "process 46-th review\n",
      "process 47-th review\n",
      "process 48-th review\n",
      "process 49-th review\n",
      "process 50-th review\n",
      "process 51-th review\n",
      "process 52-th review\n",
      "process 53-th review\n",
      "process 54-th review\n",
      "process 55-th review\n",
      "process 56-th review\n",
      "process 57-th review\n",
      "process 58-th review\n",
      "process 59-th review\n",
      "process 60-th review\n",
      "process 61-th review\n",
      "process 62-th review\n",
      "process 63-th review\n",
      "process 64-th review\n",
      "process 65-th review\n",
      "process 66-th review\n",
      "process 67-th review\n",
      "process 68-th review\n",
      "process 69-th review\n",
      "process 70-th review\n",
      "process 71-th review\n",
      "process 72-th review\n",
      "process 73-th review\n",
      "process 74-th review\n",
      "process 75-th review\n",
      "process 76-th review\n",
      "process 77-th review\n",
      "process 78-th review\n",
      "process 79-th review\n",
      "process 80-th review\n",
      "process 81-th review\n",
      "process 82-th review\n",
      "process 83-th review\n",
      "process 84-th review\n",
      "process 85-th review\n",
      "process 86-th review\n",
      "process 87-th review\n",
      "process 88-th review\n",
      "process 89-th review\n",
      "process 90-th review\n",
      "process 91-th review\n",
      "process 92-th review\n",
      "process 93-th review\n",
      "process 94-th review\n",
      "process 95-th review\n",
      "process 96-th review\n",
      "process 97-th review\n",
      "process 98-th review\n",
      "process 99-th review\n",
      "process 100-th review\n",
      "process 101-th review\n",
      "process 102-th review\n",
      "process 103-th review\n",
      "process 104-th review\n",
      "process 105-th review\n",
      "process 106-th review\n",
      "process 107-th review\n",
      "process 108-th review\n",
      "process 109-th review\n",
      "process 110-th review\n",
      "process 111-th review\n",
      "process 112-th review\n",
      "process 113-th review\n",
      "process 114-th review\n",
      "process 115-th review\n",
      "process 116-th review\n",
      "process 117-th review\n",
      "process 118-th review\n",
      "process 119-th review\n",
      "process 120-th review\n",
      "process 121-th review\n",
      "process 122-th review\n",
      "process 123-th review\n",
      "process 124-th review\n",
      "process 125-th review\n",
      "process 126-th review\n",
      "process 127-th review\n",
      "process 128-th review\n",
      "process 129-th review\n",
      "process 130-th review\n",
      "process 131-th review\n",
      "process 132-th review\n",
      "process 133-th review\n",
      "process 134-th review\n",
      "process 135-th review\n",
      "process 136-th review\n",
      "process 137-th review\n",
      "process 138-th review\n",
      "process 139-th review\n",
      "process 140-th review\n",
      "process 141-th review\n",
      "process 142-th review\n",
      "process 143-th review\n",
      "process 144-th review\n",
      "process 145-th review\n",
      "process 146-th review\n",
      "process 147-th review\n",
      "process 148-th review\n",
      "process 149-th review\n",
      "process 150-th review\n",
      "process 151-th review\n",
      "process 152-th review\n",
      "process 153-th review\n",
      "process 154-th review\n",
      "process 155-th review\n",
      "process 156-th review\n",
      "process 157-th review\n",
      "process 158-th review\n",
      "process 159-th review\n",
      "process 160-th review\n",
      "process 161-th review\n",
      "process 162-th review\n",
      "process 163-th review\n",
      "process 164-th review\n",
      "process 165-th review\n",
      "process 166-th review\n",
      "process 167-th review\n",
      "process 168-th review\n",
      "process 169-th review\n",
      "process 170-th review\n",
      "process 171-th review\n",
      "process 172-th review\n",
      "process 173-th review\n",
      "process 174-th review\n",
      "process 175-th review\n",
      "process 176-th review\n",
      "process 177-th review\n",
      "process 178-th review\n",
      "process 179-th review\n",
      "process 180-th review\n",
      "process 181-th review\n",
      "process 182-th review\n",
      "process 183-th review\n",
      "process 184-th review\n",
      "process 185-th review\n",
      "process 186-th review\n",
      "process 187-th review\n",
      "process 188-th review\n",
      "process 189-th review\n",
      "process 190-th review\n",
      "process 191-th review\n",
      "process 192-th review\n",
      "process 193-th review\n",
      "process 194-th review\n",
      "process 195-th review\n",
      "process 196-th review\n",
      "process 197-th review\n",
      "process 198-th review\n",
      "process 199-th review\n",
      "process 200-th review\n",
      "process 201-th review\n",
      "process 202-th review\n",
      "process 203-th review\n",
      "process 204-th review\n",
      "process 205-th review\n",
      "process 206-th review\n",
      "process 207-th review\n",
      "process 208-th review\n",
      "process 209-th review\n",
      "process 210-th review\n",
      "process 211-th review\n",
      "process 212-th review\n",
      "process 213-th review\n",
      "process 214-th review\n",
      "process 215-th review\n",
      "process 216-th review\n",
      "process 217-th review\n",
      "process 218-th review\n",
      "process 219-th review\n",
      "process 220-th review\n",
      "process 221-th review\n",
      "process 222-th review\n",
      "process 223-th review\n",
      "process 224-th review\n",
      "process 225-th review\n",
      "process 226-th review\n",
      "process 227-th review\n",
      "process 228-th review\n",
      "process 229-th review\n",
      "process 230-th review\n",
      "process 231-th review\n",
      "process 232-th review\n",
      "process 233-th review\n",
      "process 234-th review\n",
      "process 235-th review\n",
      "process 236-th review\n",
      "process 237-th review\n",
      "process 238-th review\n",
      "process 239-th review\n",
      "process 240-th review\n",
      "process 241-th review\n",
      "process 242-th review\n",
      "process 243-th review\n",
      "process 244-th review\n",
      "process 245-th review\n",
      "process 246-th review\n",
      "process 247-th review\n",
      "process 248-th review\n",
      "process 249-th review\n",
      "process 250-th review\n",
      "process 251-th review\n",
      "process 252-th review\n",
      "process 253-th review\n",
      "process 254-th review\n",
      "process 255-th review\n",
      "process 256-th review\n",
      "process 257-th review\n",
      "process 258-th review\n",
      "process 259-th review\n",
      "process 260-th review\n",
      "process 261-th review\n",
      "process 262-th review\n",
      "process 263-th review\n",
      "process 264-th review\n",
      "process 265-th review\n",
      "process 266-th review\n",
      "process 267-th review\n",
      "process 268-th review\n",
      "process 269-th review\n",
      "process 270-th review\n",
      "process 271-th review\n",
      "process 272-th review\n",
      "process 273-th review\n",
      "process 274-th review\n",
      "process 275-th review\n",
      "process 276-th review\n",
      "process 277-th review\n",
      "process 278-th review\n",
      "process 279-th review\n",
      "process 280-th review\n",
      "process 281-th review\n",
      "process 282-th review\n",
      "process 283-th review\n",
      "process 284-th review\n",
      "process 285-th review\n",
      "process 286-th review\n",
      "process 287-th review\n",
      "process 288-th review\n",
      "process 289-th review\n",
      "process 290-th review\n",
      "process 291-th review\n",
      "process 292-th review\n",
      "process 293-th review\n",
      "process 294-th review\n",
      "process 295-th review\n",
      "process 296-th review\n",
      "process 297-th review\n",
      "process 298-th review\n",
      "process 299-th review\n",
      "process 300-th review\n",
      "process 301-th review\n",
      "process 302-th review\n",
      "process 303-th review\n",
      "process 304-th review\n",
      "process 305-th review\n",
      "process 306-th review\n",
      "process 307-th review\n",
      "process 308-th review\n",
      "process 309-th review\n",
      "process 310-th review\n",
      "process 311-th review\n",
      "process 312-th review\n",
      "process 313-th review\n",
      "process 314-th review\n",
      "process 315-th review\n",
      "process 316-th review\n",
      "process 317-th review\n",
      "process 318-th review\n",
      "process 319-th review\n",
      "process 320-th review\n",
      "process 321-th review\n",
      "process 322-th review\n",
      "process 323-th review\n",
      "process 324-th review\n",
      "process 325-th review\n",
      "process 326-th review\n",
      "process 327-th review\n",
      "process 328-th review\n",
      "process 329-th review\n",
      "process 330-th review\n",
      "process 331-th review\n",
      "process 332-th review\n",
      "process 333-th review\n",
      "process 334-th review\n",
      "process 335-th review\n",
      "process 336-th review\n",
      "process 337-th review\n",
      "process 338-th review\n",
      "process 339-th review\n",
      "process 340-th review\n",
      "process 341-th review\n",
      "process 342-th review\n",
      "process 343-th review\n",
      "process 344-th review\n",
      "process 345-th review\n",
      "process 346-th review\n",
      "process 347-th review\n",
      "process 348-th review\n",
      "process 349-th review\n",
      "process 350-th review\n",
      "process 351-th review\n",
      "process 352-th review\n",
      "process 353-th review\n",
      "process 354-th review\n",
      "process 355-th review\n",
      "process 356-th review\n",
      "process 357-th review\n",
      "process 358-th review\n",
      "process 359-th review\n",
      "process 360-th review\n",
      "process 361-th review\n",
      "process 362-th review\n",
      "process 363-th review\n",
      "process 364-th review\n",
      "process 365-th review\n",
      "process 366-th review\n",
      "process 367-th review\n",
      "process 368-th review\n",
      "process 369-th review\n",
      "process 370-th review\n"
     ]
    }
   ],
   "source": [
    "shap_values_list = []\n",
    "token_data_list = []\n",
    "top_k = [1, 5, 9, 13]\n",
    "all_labels =[]\n",
    "# use GPU\n",
    "gpu_explainer = shap.Explainer(model_prediction_gpu, tokenizer)\n",
    "i = 0\n",
    "for review, label in zip(cor_reviews, cor_labels):\n",
    "    print(f\"process {i}-th review\")\n",
    "    i += 1\n",
    "    label4review =[]\n",
    "    label4review.append(label)\n",
    "    # to-do: truncate review if len(review)>80\n",
    "    tokens = tokenizer.tokenize(review)\n",
    "    if len(tokens) > 80:\n",
    "        tokens_truncated = tokens[:80]\n",
    "        review = \" \".join(token for token in tokens_truncated)\n",
    "    pred_label_no_mask = predict_label(pipe, review) # predicted label for review without mask\n",
    "    label4review.append(pred_label_no_mask)\n",
    "    shap_values = gpu_explainer(review)\n",
    "    values = shap_values.values[0] # 2-dim ndarray\n",
    "    returned_tokens = shap_values.data[0]\n",
    "    for k in top_k:\n",
    "        masked_review = mask_top_k(k, pred_label_no_mask, values, returned_tokens) # mask review by the shap values\n",
    "        predicted_label= predict_label(pipe, masked_review)\n",
    "        label4review.append(predicted_label)\n",
    "    # label4review = [True_label, pred_label_without_mask, masked_label_1, masked_label_2, masked_review_3, masked_review_4]\n",
    "    all_labels.append(label4review)\n",
    "    with open(\"labels.csv\", mode=\"a\") as f:\n",
    "        f.write(\"%s\\n\" % \",\".join(str(label) for label in label4review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5MItpSGr5L0"
   },
   "outputs": [],
   "source": [
    "np.save(\"all_labels.npy\", np.array(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUhh0NNnr5L0"
   },
   "outputs": [],
   "source": [
    "true_label, predicted_label_without_mask, predicted_label_mask1, predicted_label_mask5, predicted_label_mask9, predicted_label_mask13 = zip(*all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uechauf8r5L1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYTowzqIr5L1"
   },
   "outputs": [],
   "source": [
    "true_label =[int(ele) for ele in true_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cetpkYEar5L1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(np.array(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfQAq0Ytr5L1",
    "outputId": "475f7aff-0816-40af-8b15-fdd828bb1c65"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>371 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3  4  5\n",
       "0    1  0  0  0  0  0\n",
       "1    0  0  0  0  0  0\n",
       "2    0  0  1  0  0  0\n",
       "3    0  0  1  0  0  0\n",
       "4    0  0  0  0  0  0\n",
       "..  .. .. .. .. .. ..\n",
       "366  1  0  1  0  0  0\n",
       "367  1  0  0  0  0  0\n",
       "368  0  0  0  0  0  0\n",
       "369  0  0  0  0  0  0\n",
       "370  0  0  1  0  0  0\n",
       "\n",
       "[371 rows x 6 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qk9m7BtAr5L1",
    "outputId": "5e02637e-433a-497c-deab-3091a23bd34e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = df.loc[df[0]==df[1]]\n",
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g__S_Itkr5L1",
    "outputId": "0fcd1701-e70e-4ffb-cf0a-0a5d4132cbda"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>287 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3  4  5\n",
       "1    0  0  0  0  0  0\n",
       "2    0  0  1  0  0  0\n",
       "3    0  0  1  0  0  0\n",
       "4    0  0  0  0  0  0\n",
       "5    0  0  0  0  0  0\n",
       "..  .. .. .. .. .. ..\n",
       "362  0  0  0  0  0  0\n",
       "364  0  0  0  0  0  0\n",
       "368  0  0  0  0  0  0\n",
       "369  0  0  0  0  0  0\n",
       "370  0  0  1  0  0  0\n",
       "\n",
       "[287 rows x 6 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8qqtDEIr5L1",
    "outputId": "6059e302-8cfe-441d-a660-0da47435f498"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7735849056603774"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(1000, df[0], df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78284956233273\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEHou3RRr5L1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XH_fwClqr5L1",
    "outputId": "37981a43-f1b2-48f0-99af-c9d8f084d708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42160278745644597 0.9442508710801394 0.9442508710801394 0.9442508710801394\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(filtered_df[1], filtered_df[2]), accuracy_score(filtered_df[1], filtered_df[3]), accuracy_score(filtered_df[1], filtered_df[4]), accuracy_score(filtered_df[1], filtered_df[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "et6ZWgEtr5L1"
   },
   "source": [
    "Multi Class & Multi Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7I_PCdQr5L2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6B9xG7kWr5L2"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
    "                          TrainingArguments, Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhoYV1qTr5L2"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('fulldata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iiXeCehhr5L2",
    "outputId": "a5dd585d-1fad-4b4a-9fe0-96203a96dff1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ghrelin was identified in the stomach as an en...</td>\n",
       "      <td>&lt; &lt; &lt; &lt;Sustaining proliferative signaling--Rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Leptin is thought to be involved in febrigenic...</td>\n",
       "      <td>&lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PURPOSE The epidermal growth factor receptor (...</td>\n",
       "      <td>&lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt;Sustaining proliferative signal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adoptive transfer of immunity against hepatiti...</td>\n",
       "      <td>&lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt;  Avoiding immune destruc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The secretion of immunosuppressive factors lik...</td>\n",
       "      <td>&lt; &lt; &lt;Sustaining proliferative signaling--Growt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>Tumour cells primarily utilize aerobic glycoly...</td>\n",
       "      <td>&lt; &lt; &lt; &lt; &lt;Cellular energetics--Glycolysis/Warbu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>Our previous study demonstrated that 5-aminole...</td>\n",
       "      <td>&lt; &lt; &lt; &lt;Cellular energetics--Glycolysis/Warburg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>Ceramide is a sphingolipid metabolite that ind...</td>\n",
       "      <td>&lt; &lt; &lt; &lt; &lt; &lt;Resisting cell death--Necrosis&lt;Cell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>High-throughput screening of a small-molecule ...</td>\n",
       "      <td>&lt; &lt; &lt; &lt; &lt;Cellular energetics--Glycolysis/Warbu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>Aberrant glucose metabolism characterized by h...</td>\n",
       "      <td>&lt; &lt; &lt; &lt;Tumor promoting inflammation--Inflammat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1852 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Ghrelin was identified in the stomach as an en...   \n",
       "1     Leptin is thought to be involved in febrigenic...   \n",
       "2     PURPOSE The epidermal growth factor receptor (...   \n",
       "3     Adoptive transfer of immunity against hepatiti...   \n",
       "4     The secretion of immunosuppressive factors lik...   \n",
       "...                                                 ...   \n",
       "1847  Tumour cells primarily utilize aerobic glycoly...   \n",
       "1848  Our previous study demonstrated that 5-aminole...   \n",
       "1849  Ceramide is a sphingolipid metabolite that ind...   \n",
       "1850  High-throughput screening of a small-molecule ...   \n",
       "1851  Aberrant glucose metabolism characterized by h...   \n",
       "\n",
       "                                                  label  \n",
       "0     < < < <Sustaining proliferative signaling--Rec...  \n",
       "1                                    < < < < < < < < <   \n",
       "2     < < < < < < < <Sustaining proliferative signal...  \n",
       "3     < < < < < < < < < < <  Avoiding immune destruc...  \n",
       "4     < < <Sustaining proliferative signaling--Growt...  \n",
       "...                                                 ...  \n",
       "1847  < < < < <Cellular energetics--Glycolysis/Warbu...  \n",
       "1848  < < < <Cellular energetics--Glycolysis/Warburg...  \n",
       "1849  < < < < < <Resisting cell death--Necrosis<Cell...  \n",
       "1850  < < < < <Cellular energetics--Glycolysis/Warbu...  \n",
       "1851  < < < <Tumor promoting inflammation--Inflammat...  \n",
       "\n",
       "[1852 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TGCLjOPr5L2"
   },
   "outputs": [],
   "source": [
    "def find_sustain(text):\n",
    "   if text.lower().find('sustaining proliferative signaling') == -1:\n",
    "       return 0\n",
    "   else:\n",
    "       return 1\n",
    "   \n",
    "def find_evading(text):\n",
    "    if text.lower().find('evading growth suppressors') == -1:\n",
    "       return 0\n",
    "    else:\n",
    "       return 1\n",
    "   \n",
    "def find_resist(text):\n",
    "   if text.lower().find('resisting cell death') == -1:\n",
    "       return 0\n",
    "   else:\n",
    "       return 1\n",
    "   \n",
    "def find_enabling(text):\n",
    "   if text.lower().find('enabling replicative immortality') == -1:\n",
    "       return 0\n",
    "   else:\n",
    "       return 1\n",
    "def find_Inducing(text):\n",
    "   if text.lower().find('inducing angiogenesis') == -1:\n",
    "       return 0\n",
    "   else:\n",
    "       return 1\n",
    "   \n",
    "def find_Activating(text):\n",
    "   if text.lower().find('activating invasion') == -1:\n",
    "       return 0\n",
    "   else:\n",
    "       return 1\n",
    "   \n",
    "def find_Genomic(text):\n",
    "   if text.lower().find('genomic instability') == -1:\n",
    "       return 0\n",
    "   else:\n",
    "       return 1\n",
    "   \n",
    "def find_Tumor(text):\n",
    "   if text.lower().find('tumor promoting') == -1:\n",
    "       return 0\n",
    "   else:\n",
    "       return 1\n",
    "   \n",
    "def find_Cellular(text):\n",
    "   if text.lower().find('cellular energetics') == -1:\n",
    "       return 0\n",
    "   else:\n",
    "       return 1\n",
    "   \n",
    "def find_Avoiding(text):\n",
    "   if text.lower().find('avoiding immune destruction') == -1:\n",
    "       return 0\n",
    "   else:\n",
    "       return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euEKew8tr5L2"
   },
   "outputs": [],
   "source": [
    "data['Sustain proliferative signaling']=data['label'].apply(find_sustain)\n",
    "data['Evading growth suppressors']=data['label'].apply(find_evading)\n",
    "data['Resisting cell death']=data['label'].apply(find_resist)\n",
    "data['Enabling replicative immortality']=data['label'].apply(find_enabling)\n",
    "data['Inducing angiogenesis']=data['label'].apply(find_Inducing)\n",
    "data['Activating invasion and metastasis']=data['label'].apply(find_Activating)\n",
    "data['Genomic instability and mutation']=data['label'].apply(find_Genomic)\n",
    "data['Tumor promoting inflammation']=data['label'].apply(find_Tumor)\n",
    "data['Cellular energetics']=data['label'].apply(find_Cellular)\n",
    "data['Avoiding immune destruction']=data['label'].apply(find_Avoiding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_9vYbjTr5L2",
    "outputId": "edda7d0f-a07f-4925-c5e3-0ab0861dac80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                                  Ghrelin was identified in the stomach as an en...\n",
       "label                                 < < < <Sustaining proliferative signaling--Rec...\n",
       "Sustain proliferative signaling                                                     462\n",
       "Evading growth suppressors                                                          240\n",
       "Resisting cell death                                                                429\n",
       "Enabling replicative immortality                                                    115\n",
       "Inducing angiogenesis                                                               143\n",
       "Activating invasion and metastasis                                                  291\n",
       "Genomic instability and mutation                                                    333\n",
       "Tumor promoting inflammation                                                        240\n",
       "Cellular energetics                                                                 105\n",
       "Avoiding immune destruction                                                         108\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0a0zNzDr5L2"
   },
   "outputs": [],
   "source": [
    "multidataset=pd.DataFrame(data={'text':[],'Sustain proliferative signaling':[],'Evading growth suppressors':[],'Resisting cell death':[],'Enabling replicative immortality':[],'Inducing angiogenesis':[],'Activating invasion and metastasis':[],'Genomic instability and mutation':[],'Tumor promoting inflammation':[],'Cellular energetics':[],'Avoiding immune destruction':[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPekmOWlr5L2"
   },
   "outputs": [],
   "source": [
    "multidataset=data.iloc[:,[0,2,3,4,5,6,7,8,9,10,11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zKyWCHhr5L2"
   },
   "outputs": [],
   "source": [
    "multidataset.to_feather('multidataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QusOqjZyr5L2",
    "outputId": "f5a4c54b-0563-4eb7-fdf3-fb73302cbc3c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sustain proliferative signaling</th>\n",
       "      <th>Evading growth suppressors</th>\n",
       "      <th>Resisting cell death</th>\n",
       "      <th>Enabling replicative immortality</th>\n",
       "      <th>Inducing angiogenesis</th>\n",
       "      <th>Activating invasion and metastasis</th>\n",
       "      <th>Genomic instability and mutation</th>\n",
       "      <th>Tumor promoting inflammation</th>\n",
       "      <th>Cellular energetics</th>\n",
       "      <th>Avoiding immune destruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ghrelin was identified in the stomach as an en...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Leptin is thought to be involved in febrigenic...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PURPOSE The epidermal growth factor receptor (...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adoptive transfer of immunity against hepatiti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The secretion of immunosuppressive factors lik...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>Tumour cells primarily utilize aerobic glycoly...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>Our previous study demonstrated that 5-aminole...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>Ceramide is a sphingolipid metabolite that ind...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>High-throughput screening of a small-molecule ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>Aberrant glucose metabolism characterized by h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1852 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Ghrelin was identified in the stomach as an en...   \n",
       "1     Leptin is thought to be involved in febrigenic...   \n",
       "2     PURPOSE The epidermal growth factor receptor (...   \n",
       "3     Adoptive transfer of immunity against hepatiti...   \n",
       "4     The secretion of immunosuppressive factors lik...   \n",
       "...                                                 ...   \n",
       "1847  Tumour cells primarily utilize aerobic glycoly...   \n",
       "1848  Our previous study demonstrated that 5-aminole...   \n",
       "1849  Ceramide is a sphingolipid metabolite that ind...   \n",
       "1850  High-throughput screening of a small-molecule ...   \n",
       "1851  Aberrant glucose metabolism characterized by h...   \n",
       "\n",
       "      Sustain proliferative signaling  Evading growth suppressors  \\\n",
       "0                                   1                           0   \n",
       "1                                   0                           0   \n",
       "2                                   1                           1   \n",
       "3                                   0                           0   \n",
       "4                                   1                           0   \n",
       "...                               ...                         ...   \n",
       "1847                                0                           0   \n",
       "1848                                0                           0   \n",
       "1849                                0                           0   \n",
       "1850                                0                           0   \n",
       "1851                                0                           0   \n",
       "\n",
       "      Resisting cell death  Enabling replicative immortality  \\\n",
       "0                        0                                 0   \n",
       "1                        0                                 0   \n",
       "2                        1                                 0   \n",
       "3                        0                                 0   \n",
       "4                        1                                 0   \n",
       "...                    ...                               ...   \n",
       "1847                     0                                 0   \n",
       "1848                     1                                 0   \n",
       "1849                     1                                 0   \n",
       "1850                     0                                 0   \n",
       "1851                     0                                 0   \n",
       "\n",
       "      Inducing angiogenesis  Activating invasion and metastasis  \\\n",
       "0                         0                                   0   \n",
       "1                         0                                   0   \n",
       "2                         0                                   0   \n",
       "3                         0                                   0   \n",
       "4                         0                                   0   \n",
       "...                     ...                                 ...   \n",
       "1847                      0                                   0   \n",
       "1848                      0                                   0   \n",
       "1849                      0                                   0   \n",
       "1850                      0                                   0   \n",
       "1851                      0                                   1   \n",
       "\n",
       "      Genomic instability and mutation  Tumor promoting inflammation  \\\n",
       "0                                    0                             0   \n",
       "1                                    0                             0   \n",
       "2                                    0                             0   \n",
       "3                                    0                             0   \n",
       "4                                    0                             0   \n",
       "...                                ...                           ...   \n",
       "1847                                 0                             0   \n",
       "1848                                 0                             0   \n",
       "1849                                 0                             0   \n",
       "1850                                 0                             0   \n",
       "1851                                 0                             1   \n",
       "\n",
       "      Cellular energetics  Avoiding immune destruction  \n",
       "0                       0                            0  \n",
       "1                       0                            0  \n",
       "2                       0                            0  \n",
       "3                       0                            1  \n",
       "4                       0                            1  \n",
       "...                   ...                          ...  \n",
       "1847                    1                            0  \n",
       "1848                    1                            0  \n",
       "1849                    1                            0  \n",
       "1850                    1                            0  \n",
       "1851                    1                            0  \n",
       "\n",
       "[1852 rows x 11 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multidataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sD7mdfCr5L2"
   },
   "outputs": [],
   "source": [
    "multidataset.to_csv('multidataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyqFYqLWr5L3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "multitrain, multitest = train_test_split(multidataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QV-9nGNKr5L3"
   },
   "outputs": [],
   "source": [
    "multitest.reset_index().to_feather('multitest2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fN71IO13r5L3"
   },
   "outputs": [],
   "source": [
    "multitrain.reset_index().to_feather('multitrain2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kEGtq2Wr5L3",
    "outputId": "6711f8da-618b-4ff6-84fd-3d15afc2b367"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5efaad6f1e8e5747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\RmmLeo10\\.cache\\huggingface\\datasets\\csv\\default-5efaad6f1e8e5747\\0.0.0\\6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2006.36it/s]\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 8: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\RMM\\Medical1\\hoc.ipynb Cell 60'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/RMM/Medical1/hoc.ipynb#ch0000057?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/RMM/Medical1/hoc.ipynb#ch0000057?line=1'>2</a>\u001b[0m raw_datasets\u001b[39m=\u001b[39mload_dataset(\u001b[39m'\u001b[39;49m\u001b[39mcsv\u001b[39;49m\u001b[39m'\u001b[39;49m,data_files\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mmultitrain2.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/RMM/Medical1/hoc.ipynb#ch0000057?line=2'>3</a>\u001b[0m                                               \u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m'\u001b[39;49m\u001b[39mmultitest2.csv\u001b[39;49m\u001b[39m'\u001b[39;49m})\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\datasets\\load.py:1702\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, script_version, **config_kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/load.py?line=1698'>1699</a>\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/load.py?line=1700'>1701</a>\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/load.py?line=1701'>1702</a>\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/load.py?line=1702'>1703</a>\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/load.py?line=1703'>1704</a>\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/load.py?line=1704'>1705</a>\u001b[0m     ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications,\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/load.py?line=1705'>1706</a>\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/load.py?line=1706'>1707</a>\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/load.py?line=1707'>1708</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/load.py?line=1709'>1710</a>\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/load.py?line=1710'>1711</a>\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/load.py?line=1711'>1712</a>\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/load.py?line=1712'>1713</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\datasets\\builder.py:594\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=591'>592</a>\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mHF google storage unreachable. Downloading and preparing it from source\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=592'>593</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m downloaded_from_gcs:\n\u001b[1;32m--> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=593'>594</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=594'>595</a>\u001b[0m         dl_manager\u001b[39m=\u001b[39mdl_manager, verify_infos\u001b[39m=\u001b[39mverify_infos, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=595'>596</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=596'>597</a>\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=597'>598</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\datasets\\builder.py:683\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=678'>679</a>\u001b[0m split_dict\u001b[39m.\u001b[39madd(split_generator\u001b[39m.\u001b[39msplit_info)\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=680'>681</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=681'>682</a>\u001b[0m     \u001b[39m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=682'>683</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split(split_generator, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_split_kwargs)\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=683'>684</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=684'>685</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=685'>686</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot find data file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=686'>687</a>\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=687'>688</a>\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOriginal error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=688'>689</a>\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=689'>690</a>\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\datasets\\builder.py:1133\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[1;34m(self, split_generator)\u001b[0m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=1130'>1131</a>\u001b[0m generator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_tables(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39msplit_generator\u001b[39m.\u001b[39mgen_kwargs)\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=1131'>1132</a>\u001b[0m \u001b[39mwith\u001b[39;00m ArrowWriter(features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mfeatures, path\u001b[39m=\u001b[39mfpath) \u001b[39mas\u001b[39;00m writer:\n\u001b[1;32m-> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=1132'>1133</a>\u001b[0m     \u001b[39mfor\u001b[39;00m key, table \u001b[39min\u001b[39;00m utils\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=1133'>1134</a>\u001b[0m         generator, unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m tables\u001b[39m\u001b[39m\"\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, disable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m  \u001b[39m# bool(logging.get_verbosity() == logging.NOTSET)\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=1134'>1135</a>\u001b[0m     ):\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=1135'>1136</a>\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(table)\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/builder.py?line=1136'>1137</a>\u001b[0m     num_examples, num_bytes \u001b[39m=\u001b[39m writer\u001b[39m.\u001b[39mfinalize()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\tqdm\\std.py:1168\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/tqdm/std.py?line=1164'>1165</a>\u001b[0m \u001b[39m# If the bar is disabled, then just walk the iterable\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/tqdm/std.py?line=1165'>1166</a>\u001b[0m \u001b[39m# (note: keep this check outside the loop for performance)\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/tqdm/std.py?line=1166'>1167</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable:\n\u001b[1;32m-> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/tqdm/std.py?line=1167'>1168</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/tqdm/std.py?line=1168'>1169</a>\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/tqdm/std.py?line=1169'>1170</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\datasets\\packaged_modules\\csv\\csv.py:155\u001b[0m, in \u001b[0;36mCsv._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/packaged_modules/csv/csv.py?line=152'>153</a>\u001b[0m dtype \u001b[39m=\u001b[39m {name: dtype\u001b[39m.\u001b[39mto_pandas_dtype() \u001b[39mfor\u001b[39;00m name, dtype \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(schema\u001b[39m.\u001b[39mnames, schema\u001b[39m.\u001b[39mtypes)} \u001b[39mif\u001b[39;00m schema \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/packaged_modules/csv/csv.py?line=153'>154</a>\u001b[0m \u001b[39mfor\u001b[39;00m file_idx, file \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(files):\n\u001b[1;32m--> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/packaged_modules/csv/csv.py?line=154'>155</a>\u001b[0m     csv_file_reader \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file, iterator\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mread_csv_kwargs)\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/packaged_modules/csv/csv.py?line=155'>156</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/datasets/packaged_modules/csv/csv.py?line=156'>157</a>\u001b[0m         \u001b[39mfor\u001b[39;00m batch_idx, df \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(csv_file_reader):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/util/_decorators.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/util/_decorators.py?line=305'>306</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/util/_decorators.py?line=306'>307</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/util/_decorators.py?line=307'>308</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/util/_decorators.py?line=308'>309</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/util/_decorators.py?line=309'>310</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=664'>665</a>\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=665'>666</a>\u001b[0m     dialect,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=666'>667</a>\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=675'>676</a>\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=676'>677</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=677'>678</a>\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=679'>680</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=571'>572</a>\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=573'>574</a>\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=574'>575</a>\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=576'>577</a>\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=577'>578</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=929'>930</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=931'>932</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=932'>933</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1231\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=1227'>1228</a>\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=1229'>1230</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=1230'>1231</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions)\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=1231'>1232</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/readers.py?line=1232'>1233</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:75\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=71'>72</a>\u001b[0m     kwds\u001b[39m.\u001b[39mpop(key, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=73'>74</a>\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m ensure_dtype_objs(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m---> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=74'>75</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m parsers\u001b[39m.\u001b[39mTextReader(src, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=76'>77</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munnamed_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39munnamed_cols\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=78'>79</a>\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:544\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:633\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1952\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 8: invalid start byte"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets=load_dataset('csv',data_files={'train': 'multitrain.csv',\n",
    "                                              'test':'multitest.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5t3t4Z1Kr5L3",
    "outputId": "a4ccd794-d502-48e1-f6a6-0e8095bcbba5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'Sustain proliferative signaling', 'Evading growth suppressors', 'Resisting cell death', 'Enabling replicative immortality', 'Inducing angiogenesis', 'Activating invasion and metastasis', 'Genomic instability and mutation', 'Tumor promoting inflammation', 'Cellular energetics', 'Avoiding immune destruction'],\n",
       "        num_rows: 1481\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'Sustain proliferative signaling', 'Evading growth suppressors', 'Resisting cell death', 'Enabling replicative immortality', 'Inducing angiogenesis', 'Activating invasion and metastasis', 'Genomic instability and mutation', 'Tumor promoting inflammation', 'Cellular energetics', 'Avoiding immune destruction'],\n",
       "        num_rows: 371\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neWUH7Thr5L3",
    "outputId": "3d18c9a1-2502-4dfe-825d-1b1f2b02e417"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Genetic and epigenetic changes in the von Hippel-Lindau ( VHL ) tumour suppressor gene are common in sporadic conventional ( clear cell ) renal cell carcinoma ( ccRCC ) . The effects on VHL expression are unknown but increased understanding may be relevant clinically , either in terms of prognosis or in therapy selection . We have examined the expression of VHL mutant RNA in 84 ccRCC tumours previously screened for mutations in genomic DNA , 56 of which contained 52 unique mutations or polymorphisms . Based on the predicted change to the primary amino acid sequence , 24 of the mutations were missense , 11 resulted in frameshifts with premature truncation , 9 resulted in immediate truncation at the site of the mutation and 2 were frameshifts which extended the reading frame beyond the normal stop codon . Nine tumours had intronic variants , including substitution of invariant residues at splice sites , deletion of nucleotides spanning the exon-intron junction , an intronic variant of unknown function and the polymorphism c.463+43A>G . Four variants were identified which were present in genomic DNA but not in mRNA . Three of these , all encoding apparent missense changes to the primary amino acid sequence , were located close to the ends of exons , reduced the strength of the splice site and function as null rather than missense variants . One nonsense variant was not detectable in mRNA but all other mutations resulting in premature truncation codons ( PTCs ) were , suggesting truncating VHL mutations may potentially generate truncated VHL protein . An intronic variant , c.341\\\\u201111T>A , previously regarded as of unknown function , is associated with an increased level of skipping of exon 2 and may , therefore , reduce production of pVHL . Our data show that the biological consequences of VHL mutations are not necessarily predictable from the sequence change of the mutation and that for the majority of VHL mutations , the potential for the generation of mutant protein exists . ',\n",
       " 'Sustain proliferative signaling': 0,\n",
       " 'Evading growth suppressors': 0,\n",
       " 'Resisting cell death': 0,\n",
       " 'Enabling replicative immortality': 0,\n",
       " 'Inducing angiogenesis': 0,\n",
       " 'Activating invasion and metastasis': 0,\n",
       " 'Genomic instability and mutation': 1,\n",
       " 'Tumor promoting inflammation': 0,\n",
       " 'Cellular energetics': 0,\n",
       " 'Avoiding immune destruction': 0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ncBcqp5r5L3"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKCrlJyfr5L3"
   },
   "outputs": [],
   "source": [
    "# create labels column\n",
    "cols = raw_datasets[\"train\"].column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHHAmjNar5L3",
    "outputId": "37b33517-39e5-492d-c7ef-cbe7effa442c"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'to_feature'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\RMM\\Medical1\\hoc.ipynb Cell 61'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/RMM/Medical1/hoc.ipynb#ch0000062?line=0'>1</a>\u001b[0m raw_datasets\u001b[39m.\u001b[39;49mto_feature\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'to_feature'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksIq5G-Qr5L3",
    "outputId": "d92d132c-a50a-4491-f6c0-2ab4ddcdb842"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1481ex [00:00, 17157.58ex/s]\n",
      "371ex [00:00, 17726.11ex/s]\n"
     ]
    }
   ],
   "source": [
    "raw_datasets=raw_datasets.map(lambda x : {\"labels\": [x[c] for c in cols if c != \"text\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RGaAYw8r5L3",
    "outputId": "8e10008e-cd9f-43ac-8468-a511ee80f830"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Genetic and epigenetic changes in the von Hippel-Lindau ( VHL ) tumour suppressor gene are common in sporadic conventional ( clear cell ) renal cell carcinoma ( ccRCC ) . The effects on VHL expression are unknown but increased understanding may be relevant clinically , either in terms of prognosis or in therapy selection . We have examined the expression of VHL mutant RNA in 84 ccRCC tumours previously screened for mutations in genomic DNA , 56 of which contained 52 unique mutations or polymorphisms . Based on the predicted change to the primary amino acid sequence , 24 of the mutations were missense , 11 resulted in frameshifts with premature truncation , 9 resulted in immediate truncation at the site of the mutation and 2 were frameshifts which extended the reading frame beyond the normal stop codon . Nine tumours had intronic variants , including substitution of invariant residues at splice sites , deletion of nucleotides spanning the exon-intron junction , an intronic variant of unknown function and the polymorphism c.463+43A>G . Four variants were identified which were present in genomic DNA but not in mRNA . Three of these , all encoding apparent missense changes to the primary amino acid sequence , were located close to the ends of exons , reduced the strength of the splice site and function as null rather than missense variants . One nonsense variant was not detectable in mRNA but all other mutations resulting in premature truncation codons ( PTCs ) were , suggesting truncating VHL mutations may potentially generate truncated VHL protein . An intronic variant , c.341\\\\u201111T>A , previously regarded as of unknown function , is associated with an increased level of skipping of exon 2 and may , therefore , reduce production of pVHL . Our data show that the biological consequences of VHL mutations are not necessarily predictable from the sequence change of the mutation and that for the majority of VHL mutations , the potential for the generation of mutant protein exists . ',\n",
       " 'Sustain proliferative signaling': 0,\n",
       " 'Evading growth suppressors': 0,\n",
       " 'Resisting cell death': 0,\n",
       " 'Enabling replicative immortality': 0,\n",
       " 'Inducing angiogenesis': 0,\n",
       " 'Activating invasion and metastasis': 0,\n",
       " 'Genomic instability and mutation': 1,\n",
       " 'Tumor promoting inflammation': 0,\n",
       " 'Cellular energetics': 0,\n",
       " 'Avoiding immune destruction': 0,\n",
       " 'labels': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37raOqSWr5L3"
   },
   "outputs": [],
   "source": [
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, problem_type=\"multi_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3Vs37oNr5L3"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_encode(examples):\n",
    "  return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ibs-xDAr5L3",
    "outputId": "6b4acd38-4d16-41a8-eb50-596ce21d4240"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  9.21ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.92ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1481\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 371\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = raw_datasets[\"train\"].column_names\n",
    "cols.remove(\"labels\")\n",
    "ds_enc = raw_datasets.map(tokenize_and_encode, batched=True, remove_columns=cols)\n",
    "ds_enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4RCYd1Tfr5L3",
    "outputId": "4b6dbee9-fed1-4a59-be19-8bed001f977e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1481\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 371\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-x-_3kcr5L3",
    "outputId": "331adac8-35ec-45c3-91e1-b32b676e5a47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1481ex [00:00, 2448.03ex/s]\n",
      "371ex [00:00, 2885.59ex/s]\n"
     ]
    }
   ],
   "source": [
    "ds_enc.set_format(\"torch\")\n",
    "ds_enc = (ds_enc\n",
    "          .map(lambda x : {\"float_labels\": x[\"labels\"].to(torch.float)}, remove_columns=[\"labels\"])\n",
    "          .rename_column(\"float_labels\", \"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrZgTJn8r5L3",
    "outputId": "65ec7aad-c302-421e-a227-34f57355669a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels=10\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels, problem_type=\"multi_label_classification\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YW8dm8BKr5L3",
    "outputId": "b787d75c-c4b6-44da-8ed9-d52678fd38e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate = 1e-5,\n",
    "    #batch_size = 32,\n",
    "    warmup_steps= 0,\n",
    "    #max_seq_length = 128,\n",
    "    num_train_epochs = 3.0 ,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds_enc[\"train\"], eval_dataset=ds_enc[\"test\"], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbgKAjr7r5L4",
    "outputId": "febe2fdc-a48f-4eb9-a522-c5668bbf32ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RmmLeo10\\anaconda3\\envs\\Leo\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1481\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 558\n",
      " 33%|███▎      | 186/558 [00:15<00:28, 12.95it/s]***** Running Evaluation *****\n",
      "  Num examples = 371\n",
      "  Batch size = 8\n",
      "                                                 \n",
      " 34%|███▎      | 188/558 [00:16<01:31,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16554871201515198, 'eval_runtime': 1.1103, 'eval_samples_per_second': 334.148, 'eval_steps_per_second': 42.331, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 372/558 [00:32<00:13, 13.32it/s]***** Running Evaluation *****\n",
      "  Num examples = 371\n",
      "  Batch size = 8\n",
      "                                                 \n",
      " 67%|██████▋   | 374/558 [00:34<00:47,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15805183351039886, 'eval_runtime': 1.1751, 'eval_samples_per_second': 315.726, 'eval_steps_per_second': 39.998, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 500/558 [00:45<00:04, 11.75it/s]Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.13, 'learning_rate': 1.039426523297491e-06, 'epoch': 2.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./results\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./results\\checkpoint-500\\special_tokens_map.json\n",
      "100%|█████████▉| 557/558 [00:51<00:00, 11.88it/s]***** Running Evaluation *****\n",
      "  Num examples = 371\n",
      "  Batch size = 8\n",
      "                                                 \n",
      "100%|██████████| 558/558 [00:52<00:00, 11.88it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 558/558 [00:52<00:00, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1557828038930893, 'eval_runtime': 1.1741, 'eval_samples_per_second': 315.994, 'eval_steps_per_second': 40.032, 'epoch': 3.0}\n",
      "{'train_runtime': 52.2422, 'train_samples_per_second': 85.046, 'train_steps_per_second': 10.681, 'train_loss': 0.12804148444992666, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=558, training_loss=0.12804148444992666, metrics={'train_runtime': 52.2422, 'train_samples_per_second': 85.046, 'train_steps_per_second': 10.681, 'train_loss': 0.12804148444992666, 'epoch': 3.0})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTKxbQvkr5L4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CHlbjWNr5L4",
    "outputId": "da18cb4e-4f87-40dc-b8a5-92a09790e4cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 371\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [486] at entry 0 and [354] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\RMM\\Medical1\\hoc.ipynb Cell 78'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/RMM/Medical1/hoc.ipynb#ch0000082?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/RMM/Medical1/hoc.ipynb#ch0000082?line=1'>2</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/RMM/Medical1/hoc.ipynb#ch0000082?line=2'>3</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/RMM/Medical1/hoc.ipynb#ch0000082?line=5'>6</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/RMM/Medical1/hoc.ipynb#ch0000082?line=6'>7</a>\u001b[0m )\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/RMM/Medical1/hoc.ipynb#ch0000082?line=7'>8</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mevaluate()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\transformers\\trainer.py:2208\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2204'>2205</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2206'>2207</a>\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2207'>2208</a>\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2208'>2209</a>\u001b[0m     eval_dataloader,\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2209'>2210</a>\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2210'>2211</a>\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2211'>2212</a>\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2212'>2213</a>\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2213'>2214</a>\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2214'>2215</a>\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2215'>2216</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2217'>2218</a>\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2218'>2219</a>\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2219'>2220</a>\u001b[0m     speed_metrics(\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2220'>2221</a>\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2224'>2225</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2225'>2226</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\transformers\\trainer.py:2372\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2369'>2370</a>\u001b[0m observed_num_examples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2370'>2371</a>\u001b[0m \u001b[39m# Main evaluation loop\u001b[39;00m\n\u001b[1;32m-> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2371'>2372</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2372'>2373</a>\u001b[0m     \u001b[39m# Update the observed num examples\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2373'>2374</a>\u001b[0m     observed_batch_size \u001b[39m=\u001b[39m find_batch_size(inputs)\n\u001b[0;32m   <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/trainer.py?line=2374'>2375</a>\u001b[0m     \u001b[39mif\u001b[39;00m observed_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/dataloader.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/dataloader.py?line=519'>520</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/dataloader.py?line=520'>521</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/dataloader.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/dataloader.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/dataloader.py?line=523'>524</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/dataloader.py?line=524'>525</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/dataloader.py?line=558'>559</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/dataloader.py?line=559'>560</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/dataloader.py?line=560'>561</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/dataloader.py?line=561'>562</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/dataloader.py?line=562'>563</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\transformers\\data\\data_collator.py:66\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/data/data_collator.py?line=59'>60</a>\u001b[0m \u001b[39m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/data/data_collator.py?line=60'>61</a>\u001b[0m \u001b[39m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/data/data_collator.py?line=61'>62</a>\u001b[0m \u001b[39m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/data/data_collator.py?line=62'>63</a>\u001b[0m \u001b[39m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/data/data_collator.py?line=64'>65</a>\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/data/data_collator.py?line=65'>66</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/data/data_collator.py?line=66'>67</a>\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/data/data_collator.py?line=67'>68</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Leo\\lib\\site-packages\\transformers\\data\\data_collator.py:128\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/data/data_collator.py?line=125'>126</a>\u001b[0m \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabel_ids\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(v, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/data/data_collator.py?line=126'>127</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/data/data_collator.py?line=127'>128</a>\u001b[0m         batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack([f[k] \u001b[39mfor\u001b[39;49;00m f \u001b[39min\u001b[39;49;00m features])\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/data/data_collator.py?line=128'>129</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/Leo/lib/site-packages/transformers/data/data_collator.py?line=129'>130</a>\u001b[0m         batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([f[k] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features])\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [486] at entry 0 and [354] at entry 1"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_enc[\"train\"], \n",
    "    eval_dataset=ds_enc[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1645643308431,
     "user": {
      "displayName": "Ren Zexin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12623978900791661183"
     },
     "user_tz": 300
    },
    "id": "Wfq2koba-sw_"
   },
   "outputs": [],
   "source": [
    "y_pred=[[1,1,1,0,1],[1,1,1,0,1],[1,1,1,0,1]]\n",
    "y_label=[[1,0,0,1,1],[1,0,0,1,1],[1,0,0,1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1645643309700,
     "user": {
      "displayName": "Ren Zexin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12623978900791661183"
     },
     "user_tz": 300
    },
    "id": "i3s9CZpT-z1i",
    "outputId": "879ba8dd-cc04-4476-8d1d-ea4bb4e6cc14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_label, y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "hoc.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "4aa083d928abac1bb4c2de21863d23e101a74b57ce6d7b6a3a0a67f44f67e603"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
